import torch
import torch.nn as nn
import torch.nn.functional as F

# CNN Module with Batch Normalization and Dropout
class CNNModule(nn.Module):
    def __init__(self, input_channels, output_channels, kernel_size, pool_size, dropout=0.5):
        super(CNNModule, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, output_channels, kernel_size, padding=kernel_size // 2)
        self.conv2 = nn.Conv2d(output_channels, output_channels, kernel_size, padding=kernel_size // 2)
        self.pool = nn.MaxPool2d(pool_size)
        self.bn = nn.BatchNorm2d(output_channels)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        x = F.relu(self.bn(self.conv1(x)))
        x = self.pool(x)
        x = F.relu(self.bn(self.conv2(x)))
        x = self.pool(x)
        x = self.dropout(x)
        return x

# Multi-Head Self-Attention
class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super(MultiHeadSelfAttention, self).__init__()
        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)

    def forward(self, x):
        attn_output, _ = self.mha(x, x, x)
        return attn_output

# SE Block for Channel Attention
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.squeeze = nn.AdaptiveAvgPool2d(1)
        self.fc1 = nn.Linear(channels, channels // reduction, bias=False)
        self.fc2 = nn.Linear(channels // reduction, channels, bias=False)

    def forward(self, x):
        b, c, _, _ = x.size()
        z = self.squeeze(x).view(b, c)
        s = F.relu(self.fc1(z))
        s = torch.sigmoid(self.fc2(s)).view(b, c, 1, 1)
        return x * s

# Channel Attention Module
class ChannelAttentionModule(nn.Module):
    def __init__(self, input_channels):
        super(ChannelAttentionModule, self).__init__()
        self.se_block = SEBlock(input_channels)

    def forward(self, x):
        return self.se_block(x)

# Global Attention Module
class GlobalAttentionModule(nn.Module):
    def __init__(self, input_dim):
        super(GlobalAttentionModule, self).__init__()
        self.attention = nn.Linear(input_dim, input_dim)

    def forward(self, x):
        weights = torch.softmax(self.attention(x), dim=-1)
        return x * weights

# Enhanced LSTM with Dropout and Bidirectional support
class EnhancedLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=2, bidirectional=True, dropout=0.5):
        super(EnhancedLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers=num_layers, 
            bidirectional=bidirectional, 
            dropout=dropout, 
            batch_first=True
        )

    def forward(self, x):
        output, (hn, cn) = self.lstm(x)
        if self.lstm.bidirectional:
            hn = torch.cat((hn[-2], hn[-1]), dim=-1)
        else:
            hn = hn[-1]
        return hn  # Return the last hidden state

# Full Model
class AttentionModel(nn.Module):
    def __init__(self, input_channels, embed_dim, num_heads, lstm_hidden_size, dropout=0.5):
        super(AttentionModel, self).__init__()
        self.cnn = CNNModule(input_channels, output_channels=64, kernel_size=3, pool_size=2, dropout=dropout)
        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads)
        self.channel_attention = ChannelAttentionModule(input_channels=64)
        self.global_attention = GlobalAttentionModule(input_dim=embed_dim)
        self.lstm = EnhancedLSTM(embed_dim, lstm_hidden_size, num_layers=2, bidirectional=True, dropout=dropout)
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(lstm_hidden_size * 2, 1)  # Final output layer (Bidirectional LSTM)

    def forward(self, x):
        x = self.cnn(x)  # CNN module
        x = self.channel_attention(x)  # Channel Attention
        x = self.flatten(x)  # Flatten
        x = x.unsqueeze(1)  # Prepare for MHSA
        x = self.mhsa(x)  # Multi-Head Self-Attention
        x = self.lstm(x)  # LSTM layer
        x = self.global_attention(x)  # Global Attention
        x = self.fc(x)  # Final output
        return x

# Example usage
if __name__ == "__main__":
    # Example input: Batch size 8, 1 channel, 32x32 image
    input_tensor = torch.randn(8, 1, 32, 32)
    model = AttentionModel(input_channels=1, embed_dim=64, num_heads=4, lstm_hidden_size=128)
    output = model(input_tensor)
    print(output.shape)
